{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tifffile import tifffile \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General information about the data: {'description': 'Large Rocks Detection Dataset ', 'version': '1.0', 'year': 2024, 'contributor': 'Valerie Zermatten', 'date_created': '2024/09/30'}\n",
      "Number of samples  : 992\n",
      "Looking at the the first images: 2581_1126_2_2.tif\n",
      "Looking at rocks annotations for the first images:\n",
      " {'rock_id': 1459.0, 'abs_rock_coordinates': [2581767.93, 1126509.48], 'pixel_within_patch_coordinates': [608.0, 51.0], 'relative_within_patch_location': [0.95, 0.08]}\n"
     ]
    }
   ],
   "source": [
    "# to delete - Illustration of some samples coming from the dataset\n",
    "json_file_path = 'Data/large_rock_dataset.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print('General information about the data:', data['info'])\n",
    "dataset =data['dataset']\n",
    "print('Number of samples  :', len(dataset) )\n",
    "sample_info = dataset[10]\n",
    "print('Looking at the the first images:', sample_info ['file_name'])\n",
    "print('Looking at rocks annotations for the first images:\\n', sample_info ['rocks_annotations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'split': {'train', 'test'}\n"
     ]
    }
   ],
   "source": [
    "# to delete - vis\n",
    "unique_splits = set(sample['split'] for sample in dataset)\n",
    "print(\"Unique values in 'split':\", unique_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# TRAIN/TEST\n",
    "# base_dir = 'dataset_surface_hillshade'\n",
    "base_dir = 'dataset_swissimage'\n",
    "train_images_folder = os.path.join(base_dir, 'train_images')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(train_images_folder, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "## DONE:\n",
    "\n",
    "# Iterate through all samples\n",
    "# for sample in dataset:\n",
    "#     file_name = 'Data/swissImage_50cm_patches/' + sample['file_name']\n",
    "#     split = sample['split']  # Assuming the \"split\" key indicates train/test/val\n",
    "\n",
    "#     # Define source and destination paths\n",
    "#     src_path = file_name  # Assuming file_name contains the full or relative path\n",
    "#     if split == 'train':\n",
    "#         dest_dir = train_images_folder\n",
    "#     elif split == 'test':\n",
    "#         dest_dir = test_dir\n",
    "#     # elif split == 'val':\n",
    "#     #     dest_dir = val_dir\n",
    "#     else:\n",
    "#         print(f\"Unknown split '{split}' for file '{file_name}'. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     dest_path = os.path.join(dest_dir, os.path.basename(file_name))\n",
    "\n",
    "#     # Copy file to the appropriate directory\n",
    "#     try:\n",
    "#         shutil.copy(src_path, dest_path)\n",
    "#         print(f\"Copied '{file_name}' to '{dest_dir}'\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error copying '{file_name}': {e}\")\n",
    "\n",
    "# print(\"Dataset split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# train_labels\n",
    "train_labels_folder = os.path.join(base_dir, 'train_labels')\n",
    "\n",
    "# Create the train_labels directory if it doesn't exist\n",
    "os.makedirs(train_labels_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# Process images with split == 'train'\n",
    "# for sample in dataset:\n",
    "#     if sample['split'] == 'train':\n",
    "#         # Extract relevant details\n",
    "#         file_name = sample['file_name']\n",
    "#         annotations = sample.get('rocks_annotations', [])\n",
    "        \n",
    "#         # Create a .txt file for this image\n",
    "#         base_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "#         txt_file_path = os.path.join(train_labels_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "#         # Write annotations to the .txt file\n",
    "#         with open(txt_file_path, 'w') as txt_file:\n",
    "#             for annotation in annotations:\n",
    "#                 txt_file.write(f\"{annotation}\\n\")\n",
    "        \n",
    "#         print(f\"Created annotation file: {txt_file_path}\")\n",
    "\n",
    "# print(\"All train annotations have been saved to the 'train_labels' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train set: 576\n"
     ]
    }
   ],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# Create Validation Set - images\n",
    "file_count = len([file for file in os.listdir(train_images_folder) if os.path.isfile(os.path.join(train_images_folder, file))])\n",
    "print(f\"Number of images in train set: {file_count}\")\n",
    "\n",
    "# Define folder\n",
    "val_images_folder = os.path.join(base_dir, 'val_images')\n",
    "os.makedirs(val_images_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# List all files in the source folder\n",
    "# files = [file for file in os.listdir(train_images_folder) if os.path.isfile(os.path.join(train_images_folder, file))]\n",
    "\n",
    "# # Calculate 10% of the total files\n",
    "# num_files_to_move = max(1, int(len(files) * 0.1))  # Ensure at least one file is moved\n",
    "\n",
    "# # Randomly select 10% of the files\n",
    "# files_to_move = random.sample(files, num_files_to_move)\n",
    "\n",
    "# # Move the selected files\n",
    "# for file in files_to_move:\n",
    "#     src_path = os.path.join(train_images_folder, file)\n",
    "#     dest_path = os.path.join(val_images_folder, file)\n",
    "#     shutil.move(src_path, dest_path)\n",
    "#     print(f\"Moved '{file}' to '{val_images_folder}'\")\n",
    "\n",
    "# print(f\"Moved {len(files_to_move)} files to '{val_images_folder}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# Create Validation Set - labels\n",
    "val_labels_folder = os.path.join(base_dir, 'val_labels')\n",
    "os.makedirs(val_labels_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# List all image files in val_images folder (excluding extensions)\n",
    "# val_image_files = {os.path.splitext(file)[0] for file in os.listdir(val_images_folder) if os.path.isfile(os.path.join(val_images_folder, file))}\n",
    "\n",
    "# # Move matching label files from train_labels to val_labels\n",
    "# for label_file in os.listdir(train_labels_folder):\n",
    "#     # Get the base name (without extension) of the label file\n",
    "#     base_name = os.path.splitext(label_file)[0]\n",
    "    \n",
    "#     if base_name in val_image_files:\n",
    "#         src_path = os.path.join(train_labels_folder, label_file)\n",
    "#         dest_path = os.path.join(val_labels_folder, label_file)\n",
    "#         shutil.move(src_path, dest_path)\n",
    "#         print(f\"Moved '{label_file}' to '{val_labels_folder}'\")\n",
    "\n",
    "# print(\"Matching label files moved to 'val_labels' folder.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete (?) - DONE:\n",
    "\n",
    "# Path to the folder containing .tif images\n",
    "# folder_path = 'dataset_swissimage/val_images'\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.tif'):\n",
    "#         # Full path to the .tif file\n",
    "#         tif_path = os.path.join(folder_path, file)\n",
    "        \n",
    "#         # Open the .tif file\n",
    "#         try:\n",
    "#             with Image.open(tif_path) as img:\n",
    "#                 # Define the output path with the same name but .jpg extension\n",
    "#                 jpg_path = os.path.join(folder_path, file.replace('.tif', '.jpg'))\n",
    "                \n",
    "#                 # Convert and save as JPG\n",
    "#                 img.convert('RGB').save(jpg_path, 'JPEG')\n",
    "                \n",
    "#                 # Remove the original .tif file\n",
    "#                 os.remove(tif_path)\n",
    "#                 print(f\"Converted and replaced: {file} -> {jpg_path}\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# print(\"All .tif files have been converted to .jpg and replaced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete (?) - label in form for Yolo8\n",
    "\n",
    "# Input and output directories\n",
    "label_input_folder = train_labels_folder  # Folder containing original label files\n",
    "label_output_folder = os.path.join(base_dir, 'yolo_train_labels')  # Folder for YOLO-compliant labels\n",
    "os.makedirs(label_output_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# YOLOv8 assumes constant bbox size based on your description\n",
    "# bbox_width = 10 / 640  # Normalized width\n",
    "# bbox_height = 10 / 640  # Normalized height\n",
    "\n",
    "# Process each label file\n",
    "# for label_file in os.listdir(label_input_folder):\n",
    "#     if label_file.endswith('.txt'):  # Process only text files\n",
    "#         input_path = os.path.join(label_input_folder, label_file)\n",
    "#         output_path = os.path.join(label_output_folder, label_file)\n",
    "\n",
    "#         with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "#             # Read each line in the file and extract dictionaries\n",
    "#             for line in infile:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Parse the dictionary using regex\n",
    "#                 match = re.search(r\"'relative_within_patch_location': \\[(\\d+\\.\\d+), (\\d+\\.\\d+)\\]\", line)\n",
    "#                 if match:\n",
    "#                     x_center = float(match.group(1))  # Normalized x_center\n",
    "#                     y_center = float(match.group(2))  # Normalized y_center\n",
    "\n",
    "#                     # Write to YOLO format: class_id, x_center, y_center, width, height\n",
    "#                     class_id = 0  # Assuming 'rock' class is class 0\n",
    "#                     outfile.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "        \n",
    "#         print(f\"Processed: {label_file}\")\n",
    "\n",
    "# print(\"Conversion to YOLOv8 format completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADER - to put in python\n",
    "\n",
    "# Custom Dataset Class\n",
    "class RockDetectionDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        # Transforms for images\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((640, 640)), # Resize all images to 640x640\n",
    "            T.ToTensor(),  # Convert image to [0, 1] range and tensor\n",
    "        ])\n",
    "        \n",
    "        # List all images and sort to match labels\n",
    "        self.image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_folder) if f.endswith('.txt')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure 3 channels (RGB)\n",
    "        # image = self.transform(image)\n",
    "        \n",
    "        # Load label\n",
    "        label_path = os.path.join(self.label_folder, self.label_files[idx])\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                labels.append([class_id, x_center, y_center, width, height])\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "        # Apply transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "# Paths to folders\n",
    "train_images_folder = \"dataset_swissimage/train_images\"\n",
    "train_labels_folder = \"dataset_swissimage/yolo_train_labels\"\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = RockDetectionDataset(train_images_folder, train_labels_folder)\n",
    "\n",
    "# Use DataLoader to load the dataset in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating mean and standard deviation...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate mean\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating mean and standard deviation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     11\u001b[0m     images, _ \u001b[38;5;241m=\u001b[39m batch  \u001b[38;5;66;03m# Unpack the images and labels\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# images shape: [batch_size, 3, H, W]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\flore\\anaconda3\\envs\\ipeo_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    238\u001b[0m             ]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "# NORMALISATION - python \n",
    "\n",
    "# Variables to accumulate pixel statistics\n",
    "mean_sum = torch.zeros(3)\n",
    "std_sum = torch.zeros(3)\n",
    "pixel_count = 0\n",
    "\n",
    "# Calculate mean\n",
    "print(\"Calculating mean and standard deviation...\")\n",
    "for batch in train_loader:\n",
    "    images, _ = batch  # Unpack the images and labels\n",
    "    # images shape: [batch_size, 3, H, W]\n",
    "    batch_pixels = images.size(0) * images.size(2) * images.size(3)  # Number of pixels per batch\n",
    "    mean_sum += torch.sum(images, dim=[0, 2, 3])  # Sum across batch, height, width\n",
    "    std_sum += torch.sum(images ** 2, dim=[0, 2, 3])  # Sum of squared values\n",
    "    pixel_count += batch_pixels\n",
    "\n",
    "# Final mean and std calculation\n",
    "mean = mean_sum / pixel_count\n",
    "std = torch.sqrt((std_sum / pixel_count) - mean ** 2)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation (PHOUF)\n",
    "\n",
    "def transform_train_with_labels(image, label):\n",
    "    \n",
    "    # Geometric transformation applied to both image and label\n",
    "    geometric_transforms =[T.RandomVerticalFlip(p=1),\n",
    "                           T.RandomHorizontalFlip(p=1),\n",
    "                           T.RandomRotation(degrees = (90,90)),\n",
    "                           T.RandomRotation(degrees = (180,180)),\n",
    "                           T.RandomRotation(degrees = (270,270))]\n",
    "    \n",
    "    geometric_transforms_names = [\"vertical_flip\", \"horizontal_flip\", \"rotation_90\", \"rotation_180\", \"rotation_270\"]\n",
    "    \n",
    "    # Randomly select 3 transformations\n",
    "    selected_transforms = random.sample(list(zip(geometric_transforms, geometric_transforms_names)), 3)\n",
    "\n",
    "    # Extract the selected transformations and their names\n",
    "    geometric_transforms, geometric_transforms_names = zip(*selected_transforms)\n",
    "    geometric_transforms = list(geometric_transforms)\n",
    "    geometric_transforms_names = list(geometric_transforms_names)\n",
    "\n",
    "    new_images = [transform(image) for transform in geometric_transforms]\n",
    "    new_labels = [transform(label) for transform in geometric_transforms]\n",
    "    \n",
    "    # Other transformation applied only to the image\n",
    "    if random.random() < 0.5:\n",
    "      other_transforms = [T.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))]\n",
    "      other_transforms_name = [\"gaussian_blur\"]\n",
    "    else:\n",
    "      other_transforms = []\n",
    "      other_transforms_name = []\n",
    "    \n",
    "    new_images = new_images +  [transform(image) for transform in other_transforms]\n",
    "    new_labels = new_labels + [label for _ in other_transforms]     \n",
    "          \n",
    "    return new_images, new_labels, geometric_transforms_names , other_transforms_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUG - EXO 7\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# mean and standard deviation of the dataset\n",
    "mean=torch.tensor([0.504, 0.504, 0.503])\n",
    "std=torch.tensor([0.019 , 0.018, 0.018])\n",
    "\n",
    "# normalize image [0-1] (or 0-255) to zero-mean unit standard deviation\n",
    "normalize = T.Normalize(mean, std)\n",
    "# we invert normalization for plotting later\n",
    "std_inv = 1 / (std + 1e-7)\n",
    "unnormalize = T.Normalize(-mean * std_inv, std_inv)\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "  #TODO: add your own transforms here\n",
    "  T.RandomResizedCrop((200, 200)),\n",
    "  T.RandomGrayscale(),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.RandomApply([T.GaussianBlur(kernel_size=7)]),\n",
    "  T.RandomPosterize(bits=8),\n",
    "  T.RandomVerticalFlip(),\n",
    "  T.ColorJitter(),\n",
    "  T.Resize((224, 224)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\n",
    "# we do not augment the validation dataset (aside from resizing and tensor casting)\n",
    "transforms_val = T.Compose([\n",
    "  T.Resize((224, 224)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\n",
    "# Test\n",
    "dataset_index = 500\n",
    "\n",
    "img, label = dataset[dataset_index]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].imshow(unnormalize(transforms_val(img)).permute(1,2,0))\n",
    "axs[0].set_title(\"validation transform (no augmentation)\")\n",
    "\n",
    "axs[1].imshow(unnormalize(transforms_train(img)).permute(1,2,0))\n",
    "axs[1].set_title(\"training transform\")\n",
    "[ax.axis(\"off\") for ax in axs] # removes ticks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipeo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
