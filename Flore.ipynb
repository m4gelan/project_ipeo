{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tifffile import tifffile \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Illustration of some samples coming from the dataset\n",
    "json_file_path = 'Data/large_rock_dataset.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print('General information about the data:', data['info'])\n",
    "dataset =data['dataset']\n",
    "print('Number of samples  :', len(dataset) )\n",
    "sample_info = dataset[10]\n",
    "print('Looking at the the first images:', sample_info ['file_name'])\n",
    "print('Looking at rocks annotations for the first images:\\n', sample_info ['rocks_annotations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - vis\n",
    "unique_splits = set(sample['split'] for sample in dataset)\n",
    "print(\"Unique values in 'split':\", unique_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation \n",
    "\n",
    "# TRAIN/TEST\n",
    "base_dir = 'dataset_surface_hillshade'\n",
    "# base_dir = 'dataset_swissimage'\n",
    "train_images_folder = os.path.join(base_dir, 'train_images')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "# os.makedirs(base_dir, exist_ok=True)\n",
    "# os.makedirs(train_images_folder, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "## DONE:\n",
    "\n",
    "# Iterate through all samples\n",
    "# for sample in dataset:\n",
    "#     file_name = 'Data/swissImage_50cm_patches/' + sample['file_name']\n",
    "#     split = sample['split']  # Assuming the \"split\" key indicates train/test/val\n",
    "\n",
    "#     # Define source and destination paths\n",
    "#     src_path = file_name  # Assuming file_name contains the full or relative path\n",
    "#     if split == 'train':\n",
    "#         dest_dir = train_images_folder\n",
    "#     elif split == 'test':\n",
    "#         dest_dir = test_dir\n",
    "#     # elif split == 'val':\n",
    "#     #     dest_dir = val_dir\n",
    "#     else:\n",
    "#         print(f\"Unknown split '{split}' for file '{file_name}'. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     dest_path = os.path.join(dest_dir, os.path.basename(file_name))\n",
    "\n",
    "#     # Copy file to the appropriate directory\n",
    "#     try:\n",
    "#         shutil.copy(src_path, dest_path)\n",
    "#         print(f\"Copied '{file_name}' to '{dest_dir}'\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error copying '{file_name}': {e}\")\n",
    "\n",
    "# print(\"Dataset split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation \n",
    "\n",
    "# train_labels\n",
    "train_labels_folder = os.path.join(base_dir, 'train_labels')\n",
    "\n",
    "# Create the train_labels directory if it doesn't exist\n",
    "# os.makedirs(train_labels_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# Process images with split == 'train'\n",
    "# for sample in dataset:\n",
    "#     if sample['split'] == 'train':\n",
    "#         # Extract relevant details\n",
    "#         file_name = sample['file_name']\n",
    "#         annotations = sample.get('rocks_annotations', [])\n",
    "        \n",
    "#         # Create a .txt file for this image\n",
    "#         base_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "#         txt_file_path = os.path.join(train_labels_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "#         # Write annotations to the .txt file\n",
    "#         with open(txt_file_path, 'w') as txt_file:\n",
    "#             for annotation in annotations:\n",
    "#                 txt_file.write(f\"{annotation}\\n\")\n",
    "        \n",
    "#         print(f\"Created annotation file: {txt_file_path}\")\n",
    "\n",
    "# print(\"All train annotations have been saved to the 'train_labels' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation\n",
    "\n",
    "# Create Validation Set - images\n",
    "file_count = len([file for file in os.listdir(train_images_folder) if os.path.isfile(os.path.join(train_images_folder, file))])\n",
    "print(f\"Number of images in train set: {file_count}\")\n",
    "\n",
    "# Define folder\n",
    "val_images_folder = os.path.join(base_dir, 'val_images')\n",
    "# os.makedirs(val_images_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# List all files in the source folder\n",
    "# files = [file for file in os.listdir(train_images_folder) if os.path.isfile(os.path.join(train_images_folder, file))]\n",
    "\n",
    "# # Calculate 10% of the total files\n",
    "# num_files_to_move = max(1, int(len(files) * 0.1))  # Ensure at least one file is moved\n",
    "\n",
    "# # Randomly select 10% of the files\n",
    "# files_to_move = random.sample(files, num_files_to_move)\n",
    "\n",
    "# # Move the selected files\n",
    "# for file in files_to_move:\n",
    "#     src_path = os.path.join(train_images_folder, file)\n",
    "#     dest_path = os.path.join(val_images_folder, file)\n",
    "#     shutil.move(src_path, dest_path)\n",
    "#     print(f\"Moved '{file}' to '{val_images_folder}'\")\n",
    "\n",
    "# print(f\"Moved {len(files_to_move)} files to '{val_images_folder}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation \n",
    "\n",
    "# Create Validation Set - labels\n",
    "val_labels_folder = os.path.join(base_dir, 'val_labels')\n",
    "# os.makedirs(val_labels_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# List all image files in val_images folder (excluding extensions)\n",
    "# val_image_files = {os.path.splitext(file)[0] for file in os.listdir(val_images_folder) if os.path.isfile(os.path.join(val_images_folder, file))}\n",
    "\n",
    "# # Move matching label files from train_labels to val_labels\n",
    "# for label_file in os.listdir(train_labels_folder):\n",
    "#     # Get the base name (without extension) of the label file\n",
    "#     base_name = os.path.splitext(label_file)[0]\n",
    "    \n",
    "#     if base_name in val_image_files:\n",
    "#         src_path = os.path.join(train_labels_folder, label_file)\n",
    "#         dest_path = os.path.join(val_labels_folder, label_file)\n",
    "#         shutil.move(src_path, dest_path)\n",
    "#         print(f\"Moved '{label_file}' to '{val_labels_folder}'\")\n",
    "\n",
    "# print(\"Matching label files moved to 'val_labels' folder.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - tif to jpg - DONE:\n",
    "\n",
    "# Path to the folder containing .tif images\n",
    "# folder_path = 'dataset_surface_hillshade/test'\n",
    "\n",
    "# # Iterate through all files in the folder\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.tif'):\n",
    "#         # Full path to the .tif file\n",
    "#         tif_path = os.path.join(folder_path, file)\n",
    "        \n",
    "#         # Open the .tif file\n",
    "#         try:\n",
    "#             with Image.open(tif_path) as img:\n",
    "#                 # Define the output path with the same name but .jpg extension\n",
    "#                 jpg_path = os.path.join(folder_path, file.replace('.tif', '.jpg'))\n",
    "                \n",
    "#                 # Convert and save as JPG\n",
    "#                 img.convert('RGB').save(jpg_path, 'JPEG')\n",
    "                \n",
    "#                 # Remove the original .tif file\n",
    "#                 os.remove(tif_path)\n",
    "#                 print(f\"Converted and replaced: {file} -> {jpg_path}\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# print(\"All .tif files have been converted to .jpg and replaced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - label in form for Yolov8 :\n",
    "\n",
    "# Input and output directories\n",
    "label_input_folder = val_labels_folder  # Folder containing original label files\n",
    "label_output_folder = os.path.join(base_dir, 'yolo_val_labels')  # Folder for YOLO-compliant labels\n",
    "# os.makedirs(label_output_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# YOLOv8 assumes constant bbox size based on your description\n",
    "# bbox_width = 10 / 640  # Normalized width\n",
    "# bbox_height = 10 / 640  # Normalized height\n",
    "\n",
    "# # Process each label file\n",
    "# for label_file in os.listdir(label_input_folder):\n",
    "#     if label_file.endswith('.txt'):  # Process only text files\n",
    "#         input_path = os.path.join(label_input_folder, label_file)\n",
    "#         output_path = os.path.join(label_output_folder, label_file)\n",
    "\n",
    "#         with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "#             # Read each line in the file and extract dictionaries\n",
    "#             for line in infile:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Parse the dictionary using regex\n",
    "#                 match = re.search(r\"'relative_within_patch_location': \\[(\\d+\\.\\d+), (\\d+\\.\\d+)\\]\", line)\n",
    "#                 if match:\n",
    "#                     x_center = float(match.group(1))  # Normalized x_center\n",
    "#                     y_center = float(match.group(2))  # Normalized y_center\n",
    "\n",
    "#                     # Write to YOLO format: class_id, x_center, y_center, width, height\n",
    "#                     class_id = 0  # Assuming 'rock' class is class 0\n",
    "#                     outfile.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "        \n",
    "#         print(f\"Processed: {label_file}\")\n",
    "\n",
    "# print(\"Conversion to YOLOv8 format completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN - define dataset and visualise the first tensor (normalised img) - DO NOT TOUCH\n",
    "\n",
    "import dataset as dt\n",
    "\n",
    "image_folder = \"dataset_swissimage/train_images\"\n",
    "label_folder = \"dataset_swissimage/yolo_train_labels\"\n",
    "\n",
    "mean, std = dt.calculate_mean_std(image_folder)\n",
    "\n",
    "# Create Dataset\n",
    "dataset = dt.RockDetectionDataset(image_folder, label_folder, mean, std, augment=False)\n",
    "\n",
    "# Iterate through dataset\n",
    "for idx, (aug_images, aug_labels) in enumerate(dataset):\n",
    "    image_name = dataset.image_files[idx]  # Get the name of the current image\n",
    "    print(f\"Image Name: {image_name}\")\n",
    "    print(f\"Image Shape: {aug_images.size()}\")\n",
    "    print(f\"Labels: {aug_labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN - Augmentation and save augmented data - DO NOT TOUCH\n",
    "\n",
    "# Parameters\n",
    "batch_size = 16  # Adjust based on your system's memory\n",
    "base_dir = 'dataset_surface_hillshade'\n",
    "output_image_folder = os.path.join(base_dir, 'augmented_train_images')\n",
    "output_label_folder = os.path.join(base_dir, 'augmented_train_labels')\n",
    "os.makedirs(output_image_folder, exist_ok=True)\n",
    "os.makedirs(output_label_folder, exist_ok=True)\n",
    "\n",
    "# Batch processing loop\n",
    "batch_images, batch_labels, batch_names = [], [], []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    # Load original image and labels\n",
    "    original_image, original_labels = dataset[idx]\n",
    "    original_image_name = dataset.image_files[idx]  # Get image name\n",
    "\n",
    "    # Convert tensor to PIL image if needed\n",
    "    if isinstance(original_image, torch.Tensor):\n",
    "        original_image = dt.denormalize(original_image.clone(), mean, std)\n",
    "        original_image = T.ToPILImage()(original_image)\n",
    "\n",
    "    # Perform augmentations\n",
    "    augmented_images, augmented_labels = dt.transform_train_with_labels(original_image, original_labels)\n",
    "\n",
    "    # Append to batch\n",
    "    batch_images.append(augmented_images)\n",
    "    batch_labels.append(augmented_labels)\n",
    "    batch_names.append(original_image_name)\n",
    "\n",
    "    # When batch is full, save the batch\n",
    "    if len(batch_images) >= batch_size:\n",
    "        dt.save_augmented_data_batch(\n",
    "            batch_images, batch_labels, batch_names,\n",
    "            output_image_folder, output_label_folder,\n",
    "            augmentation_names=[f\"aug_{i + 1}\" for i in range(len(augmented_images))],\n",
    "            mean=mean, std=std\n",
    "        )\n",
    "        # Reset batch\n",
    "        batch_images, batch_labels, batch_names = [], [], []\n",
    "\n",
    "# Save remaining images in the last batch\n",
    "if len(batch_images) > 0:\n",
    "    dt.save_augmented_data_batch(\n",
    "        batch_images, batch_labels, batch_names,\n",
    "        output_image_folder, output_label_folder,\n",
    "        augmentation_names=[f\"aug_{i + 1}\" for i in range(len(augmented_images))],\n",
    "        mean=mean, std=std\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipeo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
