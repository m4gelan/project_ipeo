{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tifffile import tifffile \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Illustration of some samples coming from the dataset\n",
    "json_file_path = 'Data/large_rock_dataset.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print('General information about the data:', data['info'])\n",
    "dataset =data['dataset']\n",
    "print('Number of samples  :', len(dataset) )\n",
    "sample_info = dataset[10]\n",
    "print('Looking at the the first images:', sample_info ['file_name'])\n",
    "print('Looking at rocks annotations for the first images:\\n', sample_info ['rocks_annotations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - vis\n",
    "unique_splits = set(sample['split'] for sample in dataset)\n",
    "print(\"Unique values in 'split':\", unique_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# TRAIN/TEST\n",
    "base_dir = 'dataset_surface_hillshade'\n",
    "# base_dir = 'dataset_swissimage'\n",
    "train_images_folder = os.path.join(base_dir, 'train_images')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "# os.makedirs(base_dir, exist_ok=True)\n",
    "# os.makedirs(train_images_folder, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "## DONE:\n",
    "\n",
    "# Iterate through all samples\n",
    "# for sample in dataset:\n",
    "#     file_name = 'Data/swissImage_50cm_patches/' + sample['file_name']\n",
    "#     split = sample['split']  # Assuming the \"split\" key indicates train/test/val\n",
    "\n",
    "#     # Define source and destination paths\n",
    "#     src_path = file_name  # Assuming file_name contains the full or relative path\n",
    "#     if split == 'train':\n",
    "#         dest_dir = train_images_folder\n",
    "#     elif split == 'test':\n",
    "#         dest_dir = test_dir\n",
    "#     # elif split == 'val':\n",
    "#     #     dest_dir = val_dir\n",
    "#     else:\n",
    "#         print(f\"Unknown split '{split}' for file '{file_name}'. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     dest_path = os.path.join(dest_dir, os.path.basename(file_name))\n",
    "\n",
    "#     # Copy file to the appropriate directory\n",
    "#     try:\n",
    "#         shutil.copy(src_path, dest_path)\n",
    "#         print(f\"Copied '{file_name}' to '{dest_dir}'\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error copying '{file_name}': {e}\")\n",
    "\n",
    "# print(\"Dataset split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# train_labels\n",
    "train_labels_folder = os.path.join(base_dir, 'train_labels')\n",
    "\n",
    "# Create the train_labels directory if it doesn't exist\n",
    "# os.makedirs(train_labels_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# Process images with split == 'train'\n",
    "# for sample in dataset:\n",
    "#     if sample['split'] == 'train':\n",
    "#         # Extract relevant details\n",
    "#         file_name = sample['file_name']\n",
    "#         annotations = sample.get('rocks_annotations', [])\n",
    "        \n",
    "#         # Create a .txt file for this image\n",
    "#         base_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "#         txt_file_path = os.path.join(train_labels_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "#         # Write annotations to the .txt file\n",
    "#         with open(txt_file_path, 'w') as txt_file:\n",
    "#             for annotation in annotations:\n",
    "#                 txt_file.write(f\"{annotation}\\n\")\n",
    "        \n",
    "#         print(f\"Created annotation file: {txt_file_path}\")\n",
    "\n",
    "# print(\"All train annotations have been saved to the 'train_labels' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# Create Validation Set - images\n",
    "file_count = len([file for file in os.listdir(train_images_folder) if os.path.isfile(os.path.join(train_images_folder, file))])\n",
    "print(f\"Number of images in train set: {file_count}\")\n",
    "\n",
    "# Define folder\n",
    "val_images_folder = os.path.join(base_dir, 'val_images')\n",
    "# os.makedirs(val_images_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# List all files in the source folder\n",
    "# files = [file for file in os.listdir(train_images_folder) if os.path.isfile(os.path.join(train_images_folder, file))]\n",
    "\n",
    "# # Calculate 10% of the total files\n",
    "# num_files_to_move = max(1, int(len(files) * 0.1))  # Ensure at least one file is moved\n",
    "\n",
    "# # Randomly select 10% of the files\n",
    "# files_to_move = random.sample(files, num_files_to_move)\n",
    "\n",
    "# # Move the selected files\n",
    "# for file in files_to_move:\n",
    "#     src_path = os.path.join(train_images_folder, file)\n",
    "#     dest_path = os.path.join(val_images_folder, file)\n",
    "#     shutil.move(src_path, dest_path)\n",
    "#     print(f\"Moved '{file}' to '{val_images_folder}'\")\n",
    "\n",
    "# print(f\"Moved {len(files_to_move)} files to '{val_images_folder}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete - Dataset organisation for hillshade \n",
    "\n",
    "# Create Validation Set - labels\n",
    "val_labels_folder = os.path.join(base_dir, 'val_labels')\n",
    "# os.makedirs(val_labels_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# List all image files in val_images folder (excluding extensions)\n",
    "# val_image_files = {os.path.splitext(file)[0] for file in os.listdir(val_images_folder) if os.path.isfile(os.path.join(val_images_folder, file))}\n",
    "\n",
    "# # Move matching label files from train_labels to val_labels\n",
    "# for label_file in os.listdir(train_labels_folder):\n",
    "#     # Get the base name (without extension) of the label file\n",
    "#     base_name = os.path.splitext(label_file)[0]\n",
    "    \n",
    "#     if base_name in val_image_files:\n",
    "#         src_path = os.path.join(train_labels_folder, label_file)\n",
    "#         dest_path = os.path.join(val_labels_folder, label_file)\n",
    "#         shutil.move(src_path, dest_path)\n",
    "#         print(f\"Moved '{label_file}' to '{val_labels_folder}'\")\n",
    "\n",
    "# print(\"Matching label files moved to 'val_labels' folder.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete (?) - DONE:\n",
    "\n",
    "# Path to the folder containing .tif images\n",
    "# folder_path = 'dataset_swissimage/val_images'\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "# for file in os.listdir(folder_path):\n",
    "#     if file.endswith('.tif'):\n",
    "#         # Full path to the .tif file\n",
    "#         tif_path = os.path.join(folder_path, file)\n",
    "        \n",
    "#         # Open the .tif file\n",
    "#         try:\n",
    "#             with Image.open(tif_path) as img:\n",
    "#                 # Define the output path with the same name but .jpg extension\n",
    "#                 jpg_path = os.path.join(folder_path, file.replace('.tif', '.jpg'))\n",
    "                \n",
    "#                 # Convert and save as JPG\n",
    "#                 img.convert('RGB').save(jpg_path, 'JPEG')\n",
    "                \n",
    "#                 # Remove the original .tif file\n",
    "#                 os.remove(tif_path)\n",
    "#                 print(f\"Converted and replaced: {file} -> {jpg_path}\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# print(\"All .tif files have been converted to .jpg and replaced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete (?) - label in form for Yolo8\n",
    "\n",
    "# Input and output directories\n",
    "label_input_folder = val_labels_folder  # Folder containing original label files\n",
    "label_output_folder = os.path.join(base_dir, 'yolo_val_labels')  # Folder for YOLO-compliant labels\n",
    "# os.makedirs(label_output_folder, exist_ok=True)\n",
    "\n",
    "# DONE:\n",
    "\n",
    "# YOLOv8 assumes constant bbox size based on your description\n",
    "# bbox_width = 10 / 640  # Normalized width\n",
    "# bbox_height = 10 / 640  # Normalized height\n",
    "\n",
    "# # Process each label file\n",
    "# for label_file in os.listdir(label_input_folder):\n",
    "#     if label_file.endswith('.txt'):  # Process only text files\n",
    "#         input_path = os.path.join(label_input_folder, label_file)\n",
    "#         output_path = os.path.join(label_output_folder, label_file)\n",
    "\n",
    "#         with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "#             # Read each line in the file and extract dictionaries\n",
    "#             for line in infile:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Parse the dictionary using regex\n",
    "#                 match = re.search(r\"'relative_within_patch_location': \\[(\\d+\\.\\d+), (\\d+\\.\\d+)\\]\", line)\n",
    "#                 if match:\n",
    "#                     x_center = float(match.group(1))  # Normalized x_center\n",
    "#                     y_center = float(match.group(2))  # Normalized y_center\n",
    "\n",
    "#                     # Write to YOLO format: class_id, x_center, y_center, width, height\n",
    "#                     class_id = 0  # Assuming 'rock' class is class 0\n",
    "#                     outfile.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "        \n",
    "#         print(f\"Processed: {label_file}\")\n",
    "\n",
    "# print(\"Conversion to YOLOv8 format completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 640, 640])\n",
      "Labels: tensor([[0.0000, 0.4400, 0.2700, 0.0156, 0.0156],\n",
      "        [0.0000, 0.5100, 0.3900, 0.0156, 0.0156],\n",
      "        [0.0000, 0.5700, 0.4500, 0.0156, 0.0156],\n",
      "        [0.0000, 0.5700, 0.3800, 0.0156, 0.0156],\n",
      "        [0.0000, 0.3700, 0.7600, 0.0156, 0.0156],\n",
      "        [0.0000, 0.3000, 0.7100, 0.0156, 0.0156],\n",
      "        [0.0000, 0.3900, 0.9200, 0.0156, 0.0156]])\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "import dataset as dt   \n",
    "\n",
    "image_folder = \"dataset_swissimage/train_images\"\n",
    "label_folder = \"dataset_swissimage/yolo_train_labels\"\n",
    "\n",
    "mean, std = dt.calculate_mean_std(image_folder)\n",
    "\n",
    "dataset = dt.RockDetectionDataset(\n",
    "    image_folder=image_folder,\n",
    "    label_folder=label_folder,\n",
    "    mean=mean,\n",
    "    std=std\n",
    ")\n",
    "\n",
    "# Access a sample\n",
    "for img, lbl in dataset:\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Labels:\", lbl)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([4, 3, 640, 640])\n",
      "Batch labels (per image):\n",
      "  Image 0: torch.Size([0, 5])\n",
      "  Image 1: torch.Size([1, 5])\n",
      "  Image 2: torch.Size([1, 5])\n",
      "  Image 3: torch.Size([0, 5])\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=dt.custom_collate_fn)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_images, batch_labels in dataloader:\n",
    "    print(\"Batch images shape:\", batch_images.shape)\n",
    "    print(\"Batch labels (per image):\")\n",
    "    for i, labels in enumerate(batch_labels):\n",
    "        print(f\"  Image {i}: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE NORMALISED IMAGES - done \n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Folder to save the normalized images\n",
    "output_folder = \"dataset_swissimage/normalized_images\"\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Iterate through the DataLoader and save normalized images\n",
    "# for batch_idx, (batch_images, batch_labels) in enumerate(dataloader):\n",
    "#     for i, image_tensor in enumerate(batch_images):\n",
    "#         # Denormalize the image for saving\n",
    "#         denormalized_image = image_tensor.clone()  # Create a copy to modify\n",
    "#         denormalized_image[0] = denormalized_image[0] * std[0] + mean[0]  # R\n",
    "#         denormalized_image[1] = denormalized_image[1] * std[1] + mean[1]  # G\n",
    "#         denormalized_image[2] = denormalized_image[2] * std[2] + mean[2]  # B\n",
    "        \n",
    "#         # Save the image\n",
    "#         output_path = os.path.join(output_folder, f\"batch_{batch_idx + 1}_image_{i + 1}.jpg\")\n",
    "#         save_image(denormalized_image, output_path)\n",
    "    \n",
    "#     print(f\"Saved batch {batch_idx + 1} images to {output_folder}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipeo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
